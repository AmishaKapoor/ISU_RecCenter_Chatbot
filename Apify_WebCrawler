from langchain.docstore.document import Document
from langchain.indexes import VectorstoreIndexCreator
from langchain_community.utilities import ApifyWrapper
import os

os.environ["OPENAI_API_KEY"] = "----YOUR API KEY----"
os.environ["APIFY_API_TOKEN"] = "----YOUR APIFY KEY-----"

apify = ApifyWrapper()
# Call the Actor to obtain text from the crawled webpages
loader = apify.call_actor(
    actor_id="apify/website-content-crawler",
    run_input={
        "startUrls": [
           {"url": "https://campusrecreation.illinoisstate.edu"}
        ]
    },
    dataset_mapping_function=lambda item: Document(
        page_content=item["text"] or "", metadata={"source": item["url"]}
    ),
)



!pip install chromadb
!pip install tiktoken
# Create a vector store based on the crawled data
index = VectorstoreIndexCreator().from_loaders([loader])

# Query the vector store
query = "What are the gym timings?"
result = index.query(query)
print(result)
